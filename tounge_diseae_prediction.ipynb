{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages and mount Drive\n",
        "!pip install roboflow torch torchvision matplotlib seaborn scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import shutil\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive FIRST\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a dedicated folder for your tongue disease project\n",
        "project_folder = '/content/drive/MyDrive/Tongue_Disease_AI'\n",
        "os.makedirs(project_folder, exist_ok=True)\n",
        "print(f\"✅ Project folder created at: {project_folder}\")\n",
        "\n",
        "# Set dataset paths in Drive\n",
        "raw_dataset_path = os.path.join(project_folder, 'raw_dataset')\n",
        "processed_dataset_path = os.path.join(project_folder, 'processed_dataset')\n",
        "models_folder = os.path.join(project_folder, 'models')\n",
        "\n",
        "# Create necessary folders\n",
        "os.makedirs(raw_dataset_path, exist_ok=True)\n",
        "os.makedirs(processed_dataset_path, exist_ok=True)\n",
        "os.makedirs(models_folder, exist_ok=True)\n",
        "\n",
        "print(\"✅ All Drive folders created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHstbkz8oij",
        "outputId": "3994d9fa-e3cc-437d-f40c-8c0e434c9036"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.2.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.7.14)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.0.2)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.3.0)\n",
            "Collecting pi-heif<2 (from roboflow)\n",
            "  Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting pillow-avif-plugin<2 (from roboflow)\n",
            "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.5.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
            "Downloading roboflow-1.2.3-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pillow-avif-plugin, filetype, python-dotenv, pi-heif, opencv-python-headless, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, idna, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed filetype-1.2.0 idna-3.7 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencv-python-headless-4.10.0.84 pi-heif-1.0.0 pillow-avif-plugin-1.5.2 python-dotenv-1.1.1 roboflow-1.2.3\n",
            "Mounted at /content/drive\n",
            "✅ Project folder created at: /content/drive/MyDrive/Tongue_Disease_AI\n",
            "✅ All Drive folders created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download Dataset with CORRECT Format\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Your API key\n",
        "api_key = \"8Busg0IT41XLrpXmDMDL\"\n",
        "\n",
        "print(\"🔐 Using your Roboflow API key...\")\n",
        "print(\"📦 Downloading tongue disease dataset to Google Drive...\")\n",
        "\n",
        "# Change to the Drive directory before downloading\n",
        "os.chdir(raw_dataset_path)\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=api_key)\n",
        "\n",
        "# Download with CORRECT format for multilabel-classification\n",
        "project = rf.workspace(\"medical-wmypr\").project(\"tongue-tod5c\")\n",
        "dataset = project.version(1).download(\"folder\")  # Changed from \"yolov8\" to \"folder\"\n",
        "\n",
        "print(\"✅ Dataset downloaded successfully to Google Drive!\")\n",
        "\n",
        "# The dataset will be at: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/tongue-tod5c-1/\n",
        "drive_dataset_path = os.path.join(raw_dataset_path, \"tongue-tod5c-1\")\n",
        "\n",
        "print(f\"📂 Dataset location in Drive: {drive_dataset_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2FgaNSC8o0B",
        "outputId": "97c7a45e-d3b6-4840-cec7-6d623bd11ccb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔐 Using your Roboflow API key...\n",
            "📦 Downloading tongue disease dataset to Google Drive...\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "✅ Dataset downloaded successfully to Google Drive!\n",
            "📂 Dataset location in Drive: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/tongue-tod5c-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the downloaded dataset structure in Google Drive\n",
        "import os\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/tongue-tod5c-1'\n",
        "\n",
        "print(\"📊 Exploring downloaded dataset structure...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what's in the main dataset folder\n",
        "print(f\"📂 Contents of: {drive_dataset_path}\")\n",
        "if os.path.exists(drive_dataset_path):\n",
        "    main_contents = os.listdir(drive_dataset_path)\n",
        "    for item in main_contents:\n",
        "        item_path = os.path.join(drive_dataset_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            file_count = len(os.listdir(item_path))\n",
        "            print(f\"  📁 {item}/: {file_count} items\")\n",
        "        else:\n",
        "            print(f\"  📄 {item}\")\n",
        "else:\n",
        "    print(\"❌ Dataset path not found\")\n",
        "\n",
        "# Check for train/valid/test splits\n",
        "splits = ['train', 'valid', 'test']\n",
        "total_images = 0\n",
        "\n",
        "for split in splits:\n",
        "    split_path = os.path.join(drive_dataset_path, split)\n",
        "    if os.path.exists(split_path):\n",
        "        print(f\"\\n📁 {split.upper()} SET:\")\n",
        "        split_total = 0\n",
        "\n",
        "        # Check classes in this split\n",
        "        for class_folder in sorted(os.listdir(split_path)):\n",
        "            class_path = os.path.join(split_path, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                image_count = len([f for f in os.listdir(class_path)\n",
        "                                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "                if image_count > 0:  # Only show folders with images\n",
        "                    split_total += image_count\n",
        "                    print(f\"  • {class_folder}: {image_count} images\")\n",
        "\n",
        "        total_images += split_total\n",
        "        print(f\"  📊 {split} subtotal: {split_total} images\")\n",
        "\n",
        "print(f\"\\n🎯 TOTAL IMAGES: {total_images}\")\n",
        "print(f\"💾 Dataset successfully stored in Google Drive!\")\n"
      ],
      "metadata": {
        "id": "fZV6th-z9CoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3b9755-f908-42b2-9d5c-39286cf29c44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Exploring downloaded dataset structure...\n",
            "==================================================\n",
            "📂 Contents of: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/tongue-tod5c-1\n",
            "❌ Dataset path not found\n",
            "\n",
            "🎯 TOTAL IMAGES: 0\n",
            "💾 Dataset successfully stored in Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find where the dataset actually got downloaded\n",
        "import os\n",
        "\n",
        "raw_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset'\n",
        "\n",
        "print(\"🔍 Searching for the actual dataset location...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what's actually in the raw_dataset folder\n",
        "print(f\"📂 Contents of raw_dataset folder:\")\n",
        "if os.path.exists(raw_dataset_path):\n",
        "    contents = os.listdir(raw_dataset_path)\n",
        "    print(f\"Found {len(contents)} items:\")\n",
        "\n",
        "    for item in contents:\n",
        "        item_path = os.path.join(raw_dataset_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            subitem_count = len(os.listdir(item_path))\n",
        "            print(f\"  📁 {item}/: {subitem_count} items\")\n",
        "\n",
        "            # Check if this looks like our dataset\n",
        "            if subitem_count > 100:  # Likely our dataset with 10k+ files\n",
        "                print(f\"    🎯 This is likely our dataset!\")\n",
        "                actual_dataset_path = item_path\n",
        "\n",
        "                # Quick peek inside\n",
        "                subitems = os.listdir(item_path)[:10]  # First 10 items\n",
        "                print(f\"    📋 Contents: {subitems}\")\n",
        "        else:\n",
        "            print(f\"  📄 {item}\")\n",
        "else:\n",
        "    print(\"❌ Raw dataset folder not found\")\n",
        "\n",
        "# Also check the current working directory (where Roboflow downloads by default)\n",
        "print(f\"\\n🔍 Also checking current directory: {os.getcwd()}\")\n",
        "current_contents = [item for item in os.listdir('.') if 'tongue' in item.lower() or 'Tongue' in item]\n",
        "if current_contents:\n",
        "    print(f\"Found tongue-related folders in current directory: {current_contents}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCmL7DyjEh_G",
        "outputId": "8dd4dfdb-09e0-45d5-9243-4c22548ac05f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Searching for the actual dataset location...\n",
            "==================================================\n",
            "📂 Contents of raw_dataset folder:\n",
            "Found 1 items:\n",
            "  📁 Tongue-1/: 5 items\n",
            "\n",
            "🔍 Also checking current directory: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset\n",
            "Found tongue-related folders in current directory: ['Tongue-1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the actual dataset structure inside Tongue-1 folder\n",
        "import os\n",
        "\n",
        "# Update the correct dataset path\n",
        "actual_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "\n",
        "print(\"📊 Exploring Tongue-1 dataset structure...\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"📂 Dataset location: {actual_dataset_path}\")\n",
        "\n",
        "# Check the 5 items inside Tongue-1\n",
        "print(f\"\\n📋 Contents of Tongue-1:\")\n",
        "for item in os.listdir(actual_dataset_path):\n",
        "    item_path = os.path.join(actual_dataset_path, item)\n",
        "    if os.path.isdir(item_path):\n",
        "        file_count = len(os.listdir(item_path))\n",
        "        print(f\"  📁 {item}/: {file_count} items\")\n",
        "\n",
        "        # If it's a folder with many items, peek inside\n",
        "        if file_count > 10:\n",
        "            subitems = sorted(os.listdir(item_path))[:5]  # First 5 items\n",
        "            print(f\"    📋 Sample contents: {subitems}\")\n",
        "\n",
        "            # Check if these are class folders or image files\n",
        "            first_item_path = os.path.join(item_path, subitems[0])\n",
        "            if os.path.isdir(first_item_path):\n",
        "                print(f\"    📁 Contains subfolders (likely class folders)\")\n",
        "            else:\n",
        "                print(f\"    📄 Contains files directly\")\n",
        "    else:\n",
        "        print(f\"  📄 {item}\")\n",
        "\n",
        "# Check for typical dataset splits\n",
        "splits_to_check = ['train', 'valid', 'test', 'training', 'validation', 'testing']\n",
        "found_splits = []\n",
        "\n",
        "for split in splits_to_check:\n",
        "    split_path = os.path.join(actual_dataset_path, split)\n",
        "    if os.path.exists(split_path):\n",
        "        found_splits.append(split)\n",
        "\n",
        "if found_splits:\n",
        "    print(f\"\\n✅ Found dataset splits: {found_splits}\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ No standard train/valid/test splits found\")\n",
        "\n",
        "print(f\"\\n🎯 Ready to analyze the dataset structure!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g96nfiDLFUTN",
        "outputId": "01724c60-c906-4bed-955d-f61b332943ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Exploring Tongue-1 dataset structure...\n",
            "==================================================\n",
            "📂 Dataset location: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1\n",
            "\n",
            "📋 Contents of Tongue-1:\n",
            "  📄 README.dataset.txt\n",
            "  📄 README.roboflow.txt\n",
            "  📁 test/: 116 items\n",
            "    📋 Sample contents: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_thin', 'colorResult_grey shapeResult_ToothMarks textureResult_normal thicknessResult_greasy']\n",
            "    📁 Contains subfolders (likely class folders)\n",
            "  📁 train/: 166 items\n",
            "    📋 Sample contents: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_thin', 'colorResult_grey shapeResult_ToothMarks textureResult_normal thicknessResult_Stripping']\n",
            "    📁 Contains subfolders (likely class folders)\n",
            "  📁 valid/: 120 items\n",
            "    📋 Sample contents: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_thin', 'colorResult_grey shapeResult_ToothMarks textureResult_tender thicknessResult_greasy']\n",
            "    📁 Contains subfolders (likely class folders)\n",
            "\n",
            "✅ Found dataset splits: ['train', 'valid', 'test']\n",
            "\n",
            "🎯 Ready to analyze the dataset structure!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count actual images in each split and analyze the class combinations\n",
        "import os\n",
        "\n",
        "actual_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "\n",
        "print(\"📊 Analyzing tongue disease dataset - Image counts per split...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "total_images = 0\n",
        "all_classes = set()\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_path = os.path.join(actual_dataset_path, split)\n",
        "    split_images = 0\n",
        "    split_classes = 0\n",
        "\n",
        "    print(f\"\\n📁 {split.upper()} SET:\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    class_folders = sorted(os.listdir(split_path))\n",
        "\n",
        "    for class_folder in class_folders:\n",
        "        class_path = os.path.join(split_path, class_folder)\n",
        "        if os.path.isdir(class_path):\n",
        "            # Count images in this class\n",
        "            image_files = [f for f in os.listdir(class_path)\n",
        "                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]\n",
        "            image_count = len(image_files)\n",
        "\n",
        "            if image_count > 0:\n",
        "                split_images += image_count\n",
        "                split_classes += 1\n",
        "                all_classes.add(class_folder)\n",
        "\n",
        "                # Show first few classes as examples\n",
        "                if split_classes <= 3:\n",
        "                    print(f\"  • {class_folder}: {image_count} images\")\n",
        "\n",
        "    # Show summary for this split\n",
        "    if split_classes > 3:\n",
        "        print(f\"  ... and {split_classes - 3} more classes\")\n",
        "\n",
        "    print(f\"  📊 {split} Summary: {split_images} images across {split_classes} classes\")\n",
        "    total_images += split_images\n",
        "\n",
        "print(f\"\\n🎯 DATASET SUMMARY:\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"📈 Total Images: {total_images}\")\n",
        "print(f\"🏷️ Total Unique Classes: {len(all_classes)}\")\n",
        "print(f\"📊 Perfect for multi-attribute tongue diagnosis!\")\n",
        "\n",
        "# Analyze the attribute combinations\n",
        "print(f\"\\n🔍 Sample class combinations (first 5):\")\n",
        "sample_classes = list(all_classes)[:5]\n",
        "for i, class_name in enumerate(sample_classes, 1):\n",
        "    print(f\"{i}. {class_name}\")\n",
        "\n",
        "print(f\"\\n✅ Dataset ready for advanced tongue disease classification!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJyHzzCpFep0",
        "outputId": "e506ccd8-dabc-466e-e53c-132cebb97218"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Analyzing tongue disease dataset - Image counts per split...\n",
            "============================================================\n",
            "\n",
            "📁 TRAIN SET:\n",
            "------------------------------\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping: 4 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis: 43 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy: 34 images\n",
            "  ... and 163 more classes\n",
            "  📊 train Summary: 7929 images across 166 classes\n",
            "\n",
            "📁 VALID SET:\n",
            "------------------------------\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping: 1 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis: 3 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy: 3 images\n",
            "  ... and 117 more classes\n",
            "  📊 valid Summary: 973 images across 120 classes\n",
            "\n",
            "📁 TEST SET:\n",
            "------------------------------\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping: 1 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis: 9 images\n",
            "  • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy: 5 images\n",
            "  ... and 113 more classes\n",
            "  📊 test Summary: 988 images across 116 classes\n",
            "\n",
            "🎯 DATASET SUMMARY:\n",
            "==============================\n",
            "📈 Total Images: 9890\n",
            "🏷️ Total Unique Classes: 171\n",
            "📊 Perfect for multi-attribute tongue diagnosis!\n",
            "\n",
            "🔍 Sample class combinations (first 5):\n",
            "1. colorResult_yellow shapeResult_fat textureResult_dark thicknessResult_greasy\n",
            "2. colorResult_white shapeResult_fat textureResult_dark thicknessResult_thin\n",
            "3. colorResult_grey shapeResult_thin textureResult_normal thicknessResult_ecchymosis\n",
            "4. colorResult_white shapeResult_normal textureResult_normal thicknessResult_greasy\n",
            "5. colorResult_yellow shapeResult_fat textureResult_tender thicknessResult_Stripping\n",
            "\n",
            "✅ Dataset ready for advanced tongue disease classification!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training pipeline for the multi-attribute tongue disease dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"🚀 Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "# Enhanced data transformations for medical images\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.2),  # Conservative for medical data\n",
        "    transforms.RandomRotation(5),  # Small rotation for tongues\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load datasets from Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "valid_path = os.path.join(dataset_path, 'valid')\n",
        "test_path = os.path.join(dataset_path, 'test')\n",
        "\n",
        "print(\"📚 Loading multi-attribute tongue disease dataset...\")\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "val_dataset = datasets.ImageFolder(root=valid_path, transform=transform_val)\n",
        "test_dataset = datasets.ImageFolder(root=test_path, transform=transform_val)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32  # Good for this dataset size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Get dataset information\n",
        "num_classes = len(train_dataset.classes)\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "print(f\"✅ Professional dataset loaded successfully!\")\n",
        "print(f\"📈 Training samples: {len(train_dataset)}\")\n",
        "print(f\"📊 Validation samples: {len(val_dataset)}\")\n",
        "print(f\"🧪 Test samples: {len(test_dataset)}\")\n",
        "print(f\"🏷️ Number of classes: {num_classes}\")\n",
        "print(f\"📝 Multi-attribute tongue diagnosis ready!\")\n",
        "\n",
        "# Show sample class names to verify\n",
        "print(f\"\\n🔍 Sample classes (first 3):\")\n",
        "for i, class_name in enumerate(class_names[:3]):\n",
        "    print(f\"{i+1}. {class_name}\")\n",
        "\n",
        "print(f\"\\n🎯 Ready for DenseNet-121 model initialization!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVKPD3jeFriJ",
        "outputId": "98737960-e5c3-4e7b-8273-8329fc732322"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n",
            "📚 Loading multi-attribute tongue disease dataset...\n",
            "✅ Professional dataset loaded successfully!\n",
            "📈 Training samples: 7929\n",
            "📊 Validation samples: 973\n",
            "🧪 Test samples: 988\n",
            "🏷️ Number of classes: 166\n",
            "📝 Multi-attribute tongue diagnosis ready!\n",
            "\n",
            "🔍 Sample classes (first 3):\n",
            "1. colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping\n",
            "2. colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis\n",
            "3. colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_greasy\n",
            "\n",
            "🎯 Ready for DenseNet-121 model initialization!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DenseNet-121 model for 166-class multi-attribute tongue diagnosis\n",
        "class TongueDiseaseClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(TongueDiseaseClassifier, self).__init__()\n",
        "\n",
        "        # Load pre-trained DenseNet-121\n",
        "        self.densenet = models.densenet121(pretrained=pretrained)\n",
        "\n",
        "        # Get number of features from the original classifier\n",
        "        num_features = self.densenet.classifier.in_features\n",
        "\n",
        "        # Custom classifier for multi-attribute tongue diagnosis\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_features, 1024),  # Larger layer for complex classification\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, num_classes)  # 166 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.densenet(x)\n",
        "\n",
        "# Initialize model\n",
        "print(\"🤖 Initializing DenseNet-121 for multi-attribute tongue diagnosis...\")\n",
        "\n",
        "model = TongueDiseaseClassifier(num_classes=num_classes, pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training components optimized for multi-class medical data\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=7, verbose=True)\n",
        "\n",
        "# Model information\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"✅ Multi-attribute classifier initialized successfully!\")\n",
        "print(f\"📊 Total parameters: {total_params:,}\")\n",
        "print(f\"🎯 Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"🏗️ Architecture: DenseNet-121 → 1024 → 512 → {num_classes} classes\")\n",
        "print(f\"🧠 Capable of predicting: Color + Shape + Texture + Thickness\")\n",
        "print(f\"💪 Model ready for training on {len(train_dataset)} medical images!\")\n",
        "\n",
        "print(f\"\\n🚀 Ready for training! This will be a sophisticated medical AI system!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjVLsdwhF9wa",
        "outputId": "b46e3ccb-1bce-4192-c63f-0b5b7bb3ac67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 Initializing DenseNet-121 for multi-attribute tongue diagnosis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 146MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Multi-attribute classifier initialized successfully!\n",
            "📊 Total parameters: 8,613,414\n",
            "🎯 Trainable parameters: 8,613,414\n",
            "🏗️ Architecture: DenseNet-121 → 1024 → 512 → 166 classes\n",
            "🧠 Capable of predicting: Color + Shape + Texture + Thickness\n",
            "💪 Model ready for training on 7929 medical images!\n",
            "\n",
            "🚀 Ready for training! This will be a sophisticated medical AI system!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simplified dataset using the color_shape strategy (12 classes - optimal balance)\n",
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def create_simplified_dataset(source_path, target_path, strategy='color_shape'):\n",
        "    \"\"\"Create simplified dataset based on chosen strategy\"\"\"\n",
        "\n",
        "    print(f\"🔄 Creating simplified dataset using '{strategy}' strategy...\")\n",
        "\n",
        "    # Define the simplification function\n",
        "    strategies = {\n",
        "        'color_only': lambda color, shape, texture, thickness: f\"color_{color}\",\n",
        "        'shape_only': lambda color, shape, texture, thickness: f\"shape_{shape}\",\n",
        "        'color_shape': lambda color, shape, texture, thickness: f\"color_{color}_shape_{shape}\",\n",
        "        'texture_thickness': lambda color, shape, texture, thickness: f\"texture_{texture}_thickness_{thickness}\"\n",
        "    }\n",
        "\n",
        "    simplify_func = strategies[strategy]\n",
        "\n",
        "    # Process each split\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        source_split_path = os.path.join(source_path, split)\n",
        "        target_split_path = os.path.join(target_path, split)\n",
        "\n",
        "        if os.path.exists(source_split_path):\n",
        "            print(f\"\\n📂 Processing {split} split...\")\n",
        "\n",
        "            # Count images per simplified class\n",
        "            simplified_counts = defaultdict(int)\n",
        "\n",
        "            # First pass: count what we'll have\n",
        "            for class_folder in os.listdir(source_split_path):\n",
        "                class_path = os.path.join(source_split_path, class_folder)\n",
        "                if os.path.isdir(class_path):\n",
        "                    # Extract attributes\n",
        "                    color, shape, texture, thickness = extract_primary_attributes(class_folder)\n",
        "                    simplified_class = simplify_func(color, shape, texture, thickness)\n",
        "\n",
        "                    # Count images in this original class\n",
        "                    image_count = len([f for f in os.listdir(class_path)\n",
        "                                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    simplified_counts[simplified_class] += image_count\n",
        "\n",
        "            # Second pass: copy images to simplified structure\n",
        "            for class_folder in os.listdir(source_split_path):\n",
        "                class_path = os.path.join(source_split_path, class_folder)\n",
        "                if os.path.isdir(class_path):\n",
        "                    # Extract attributes\n",
        "                    color, shape, texture, thickness = extract_primary_attributes(class_folder)\n",
        "                    simplified_class = simplify_func(color, shape, texture, thickness)\n",
        "\n",
        "                    # Create target directory\n",
        "                    target_class_path = os.path.join(target_split_path, simplified_class)\n",
        "                    os.makedirs(target_class_path, exist_ok=True)\n",
        "\n",
        "                    # Copy all images from this class\n",
        "                    for image_file in os.listdir(class_path):\n",
        "                        if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                            source_image = os.path.join(class_path, image_file)\n",
        "                            # Create unique filename to avoid conflicts\n",
        "                            unique_filename = f\"{class_folder}_{image_file}\"\n",
        "                            target_image = os.path.join(target_class_path, unique_filename)\n",
        "                            shutil.copy2(source_image, target_image)\n",
        "\n",
        "            # Show results for this split\n",
        "            print(f\"  ✅ {split} simplified classes:\")\n",
        "            for class_name, count in sorted(simplified_counts.items()):\n",
        "                print(f\"    • {class_name}: {count} images\")\n",
        "\n",
        "    return target_path\n",
        "\n",
        "# Create the simplified dataset\n",
        "original_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "simplified_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset'\n",
        "\n",
        "# Use color_shape strategy (12 classes - good balance)\n",
        "created_path = create_simplified_dataset(\n",
        "    original_dataset_path,\n",
        "    simplified_dataset_path,\n",
        "    strategy='color_shape'\n",
        ")\n",
        "\n",
        "print(f\"\\n🎯 SIMPLIFIED DATASET SUMMARY:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Analyze the final simplified dataset\n",
        "total_images = 0\n",
        "total_classes = 0\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_path = os.path.join(simplified_dataset_path, split)\n",
        "    if os.path.exists(split_path):\n",
        "        split_classes = 0\n",
        "        split_images = 0\n",
        "\n",
        "        for class_folder in os.listdir(split_path):\n",
        "            class_path = os.path.join(split_path, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                image_count = len([f for f in os.listdir(class_path)\n",
        "                                 if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                split_classes += 1\n",
        "                split_images += image_count\n",
        "\n",
        "        total_classes = split_classes  # Should be same across splits\n",
        "        total_images += split_images\n",
        "        print(f\"📊 {split}: {split_images} images across {split_classes} classes\")\n",
        "\n",
        "print(f\"\\n🏆 FINAL RESULTS:\")\n",
        "print(f\"📈 Total Images: {total_images}\")\n",
        "print(f\"🏷️ Total Classes: {total_classes}\")\n",
        "print(f\"📊 Average images per class: {total_images//total_classes if total_classes > 0 else 0}\")\n",
        "print(f\"💾 Simplified dataset saved at: {simplified_dataset_path}\")\n",
        "\n",
        "print(f\"\\n✅ Ready for retraining with much better performance expected!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "-qG7GpZ2WQ0r",
        "outputId": "a96b4bff-7d68-4cc9-a027-3732dbba9aac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Creating simplified dataset using 'color_shape' strategy...\n",
            "\n",
            "📂 Processing train split...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_primary_attributes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-953160559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;31m# Use color_shape strategy (12 classes - good balance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m created_path = create_simplified_dataset(\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0moriginal_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msimplified_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-953160559.py\u001b[0m in \u001b[0;36mcreate_simplified_dataset\u001b[0;34m(source_path, target_path, strategy)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0;31m# Extract attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_primary_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0msimplified_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplify_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_primary_attributes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debug the dataset creation process to identify the issue\n",
        "import os\n",
        "\n",
        "print(\"🔍 Debugging dataset creation issue...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check if source paths exist\n",
        "original_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "simplified_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset'\n",
        "\n",
        "print(\"📂 Checking source dataset paths...\")\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    source_split_path = os.path.join(original_dataset_path, split)\n",
        "    print(f\"  {split}: {os.path.exists(source_split_path)} - {source_split_path}\")\n",
        "\n",
        "    if os.path.exists(source_split_path):\n",
        "        class_count = len([d for d in os.listdir(source_split_path)\n",
        "                          if os.path.isdir(os.path.join(source_split_path, d))])\n",
        "        print(f\"    └── {class_count} class folders found\")\n",
        "\n",
        "# Check if the extract_primary_attributes function is working\n",
        "print(f\"\\n🔧 Testing attribute extraction...\")\n",
        "if os.path.exists(os.path.join(original_dataset_path, 'train')):\n",
        "    sample_classes = os.listdir(os.path.join(original_dataset_path, 'train'))[:3]\n",
        "    for sample_class in sample_classes:\n",
        "        color, shape, texture, thickness = extract_primary_attributes(sample_class)\n",
        "        print(f\"  Class: {sample_class}\")\n",
        "        print(f\"    → Color: {color}, Shape: {shape}, Texture: {texture}, Thickness: {thickness}\")\n",
        "\n",
        "# Check what was actually created in simplified dataset\n",
        "print(f\"\\n📊 Checking simplified dataset contents...\")\n",
        "if os.path.exists(simplified_dataset_path):\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        split_path = os.path.join(simplified_dataset_path, split)\n",
        "        if os.path.exists(split_path):\n",
        "            contents = os.listdir(split_path)\n",
        "            print(f\"  {split}: {len(contents)} items - {contents[:5]}\")\n",
        "        else:\n",
        "            print(f\"  {split}: folder doesn't exist\")\n",
        "else:\n",
        "    print(\"  ❌ Simplified dataset folder doesn't exist\")\n",
        "\n",
        "print(f\"\\n🎯 Diagnosis complete - ready for fix!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ObXMZ6xJWd32",
        "outputId": "1c3a5c01-d9a5-466c-a88b-be9f98afc576"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Debugging dataset creation issue...\n",
            "==================================================\n",
            "📂 Checking source dataset paths...\n",
            "  train: True - /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1/train\n",
            "    └── 166 class folders found\n",
            "  valid: True - /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1/valid\n",
            "    └── 120 class folders found\n",
            "  test: True - /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1/test\n",
            "    └── 116 class folders found\n",
            "\n",
            "🔧 Testing attribute extraction...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_primary_attributes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-1187144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msample_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_dataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msample_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_primary_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Class: {sample_class}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"    → Color: {color}, Shape: {shape}, Texture: {texture}, Thickness: {thickness}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_primary_attributes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find where the dataset actually exists and fix the paths\n",
        "import os\n",
        "\n",
        "print(\"🔍 Searching for the actual dataset location...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Search in the raw_dataset folder\n",
        "raw_dataset_base = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset'\n",
        "\n",
        "if os.path.exists(raw_dataset_base):\n",
        "    print(f\"📂 Contents of raw_dataset folder:\")\n",
        "    for item in os.listdir(raw_dataset_base):\n",
        "        item_path = os.path.join(raw_dataset_base, item)\n",
        "        print(f\"  📁 {item}\")\n",
        "\n",
        "        # Check if this folder contains train/valid/test\n",
        "        if os.path.isdir(item_path):\n",
        "            subitems = os.listdir(item_path)\n",
        "            has_splits = any(split in subitems for split in ['train', 'valid', 'test'])\n",
        "            if has_splits:\n",
        "                print(f\"    ✅ Contains dataset splits: {[s for s in subitems if s in ['train', 'valid', 'test']]}\")\n",
        "\n",
        "                # This is likely our dataset - check image counts\n",
        "                for split in ['train', 'valid', 'test']:\n",
        "                    split_path = os.path.join(item_path, split)\n",
        "                    if os.path.exists(split_path):\n",
        "                        class_folders = [d for d in os.listdir(split_path)\n",
        "                                       if os.path.isdir(os.path.join(split_path, d))]\n",
        "                        total_images = 0\n",
        "                        for cf in class_folders[:3]:  # Check first 3 classes\n",
        "                            cf_path = os.path.join(split_path, cf)\n",
        "                            img_count = len([f for f in os.listdir(cf_path)\n",
        "                                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                            total_images += img_count\n",
        "\n",
        "                        print(f\"      📊 {split}: {len(class_folders)} classes, ~{total_images} images (from first 3 classes)\")\n",
        "\n",
        "                        # Show sample class names\n",
        "                        if class_folders:\n",
        "                            print(f\"      📝 Sample classes: {class_folders[:2]}\")\n",
        "\n",
        "                # This is our correct dataset path!\n",
        "                correct_dataset_path = item_path\n",
        "                print(f\"\\n🎯 FOUND DATASET AT: {correct_dataset_path}\")\n",
        "            else:\n",
        "                print(f\"    📄 Contains: {subitems[:3]}...\")\n",
        "\n",
        "print(f\"\\n✅ Dataset location identified!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhbXl-KXXSQr",
        "outputId": "86da4bf2-f733-4f74-aba1-7f5dc140907f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Searching for the actual dataset location...\n",
            "==================================================\n",
            "📂 Contents of raw_dataset folder:\n",
            "  📁 Tongue-1\n",
            "    ✅ Contains dataset splits: ['test', 'train', 'valid']\n",
            "      📊 train: 166 classes, ~81 images (from first 3 classes)\n",
            "      📝 Sample classes: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis']\n",
            "      📊 valid: 120 classes, ~7 images (from first 3 classes)\n",
            "      📝 Sample classes: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis']\n",
            "      📊 test: 116 classes, ~15 images (from first 3 classes)\n",
            "      📝 Sample classes: ['colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping', 'colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis']\n",
            "\n",
            "🎯 FOUND DATASET AT: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1\n",
            "\n",
            "✅ Dataset location identified!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually explore the raw_dataset folder to find where the tongue dataset is located\n",
        "import os\n",
        "\n",
        "print(\"🔍 Manual exploration of raw_dataset folder...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "raw_dataset_base = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset'\n",
        "\n",
        "def explore_folder_deeply(folder_path, max_depth=3, current_depth=0):\n",
        "    \"\"\"Recursively explore folders to find the dataset\"\"\"\n",
        "    if current_depth >= max_depth or not os.path.exists(folder_path):\n",
        "        return None\n",
        "\n",
        "    indent = \"  \" * current_depth\n",
        "    items = os.listdir(folder_path)\n",
        "\n",
        "    print(f\"{indent}📁 {os.path.basename(folder_path)}/ ({len(items)} items)\")\n",
        "\n",
        "    # Check if current folder has train/valid/test structure\n",
        "    has_train = 'train' in items\n",
        "    has_valid = 'valid' in items or 'validation' in items\n",
        "    has_test = 'test' in items\n",
        "\n",
        "    if has_train and (has_valid or has_test):\n",
        "        print(f\"{indent}🎯 DATASET FOUND! This folder has train/valid/test structure\")\n",
        "\n",
        "        # Count images in each split\n",
        "        for split in ['train', 'valid', 'test']:\n",
        "            split_path = os.path.join(folder_path, split)\n",
        "            if os.path.exists(split_path):\n",
        "                # Count class folders\n",
        "                class_folders = [d for d in os.listdir(split_path)\n",
        "                               if os.path.isdir(os.path.join(split_path, d))]\n",
        "\n",
        "                # Count total images\n",
        "                total_images = 0\n",
        "                for class_folder in class_folders[:5]:  # Check first 5 classes\n",
        "                    class_path = os.path.join(split_path, class_folder)\n",
        "                    image_count = len([f for f in os.listdir(class_path)\n",
        "                                     if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    total_images += image_count\n",
        "\n",
        "                print(f\"{indent}  📊 {split}: {len(class_folders)} classes, ~{total_images * len(class_folders) // 5 if class_folders else 0} total images\")\n",
        "\n",
        "                # Show sample class names\n",
        "                if class_folders:\n",
        "                    print(f\"{indent}  📝 Sample classes:\")\n",
        "                    for class_name in class_folders[:2]:\n",
        "                        print(f\"{indent}    • {class_name}\")\n",
        "\n",
        "        return folder_path\n",
        "\n",
        "    # Explore subdirectories\n",
        "    for item in items:\n",
        "        item_path = os.path.join(folder_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"{indent}  📁 {item}/\")\n",
        "            result = explore_folder_deeply(item_path, max_depth, current_depth + 1)\n",
        "            if result:\n",
        "                return result\n",
        "        else:\n",
        "            # Show first few files\n",
        "            if items.index(item) < 3:\n",
        "                print(f\"{indent}  📄 {item}\")\n",
        "\n",
        "    if len(items) > 3:\n",
        "        file_count = len([i for i in items if os.path.isfile(os.path.join(folder_path, i))])\n",
        "        if file_count > 3:\n",
        "            print(f\"{indent}  ... and {file_count - 3} more files\")\n",
        "\n",
        "    return None\n",
        "\n",
        "print(f\"🔍 Exploring: {raw_dataset_base}\")\n",
        "\n",
        "if os.path.exists(raw_dataset_base):\n",
        "    dataset_location = explore_folder_deeply(raw_dataset_base)\n",
        "\n",
        "    if dataset_location:\n",
        "        print(f\"\\n🎯 DATASET FOUND AT: {dataset_location}\")\n",
        "        print(f\"✅ This is where your 9,890 tongue images are stored!\")\n",
        "\n",
        "        # Store the path for next steps\n",
        "        print(f\"\\n📋 Use this path for simplified dataset creation:\")\n",
        "        print(f\"correct_dataset_path = '{dataset_location}'\")\n",
        "    else:\n",
        "        print(f\"\\n❌ No dataset with train/valid/test structure found\")\n",
        "        print(f\"🔍 Let's check if files are in a different structure...\")\n",
        "\n",
        "        # Alternative: look for any folder with many images\n",
        "        print(f\"\\n🔍 Looking for folders with many images...\")\n",
        "        for item in os.listdir(raw_dataset_base):\n",
        "            item_path = os.path.join(raw_dataset_base, item)\n",
        "            if os.path.isdir(item_path):\n",
        "                # Count total files recursively\n",
        "                total_files = 0\n",
        "                for root, dirs, files in os.walk(item_path):\n",
        "                    total_files += len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "                if total_files > 100:  # Likely our dataset\n",
        "                    print(f\"  📊 {item}: {total_files} image files\")\n",
        "                    print(f\"      Path: {item_path}\")\n",
        "else:\n",
        "    print(f\"❌ Raw dataset folder doesn't exist: {raw_dataset_base}\")\n",
        "\n",
        "    # Check if the base project folder exists\n",
        "    project_base = '/content/drive/MyDrive/Tongue_Disease_AI'\n",
        "    if os.path.exists(project_base):\n",
        "        print(f\"\\n📂 Contents of project folder:\")\n",
        "        for item in os.listdir(project_base):\n",
        "            print(f\"  📁 {item}\")\n",
        "    else:\n",
        "        print(f\"❌ Project folder doesn't exist either!\")\n",
        "\n",
        "print(f\"\\n🎯 Manual exploration complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35yP_theZENX",
        "outputId": "105ce0e1-73f4-4b52-d8b1-595efbfa074c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Manual exploration of raw_dataset folder...\n",
            "============================================================\n",
            "🔍 Exploring: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset\n",
            "📁 raw_dataset/ (1 items)\n",
            "  📁 Tongue-1/\n",
            "  📁 Tongue-1/ (5 items)\n",
            "  🎯 DATASET FOUND! This folder has train/valid/test structure\n",
            "    📊 train: 166 classes, ~3220 total images\n",
            "    📝 Sample classes:\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis\n",
            "    📊 valid: 120 classes, ~240 total images\n",
            "    📝 Sample classes:\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis\n",
            "    📊 test: 116 classes, ~394 total images\n",
            "    📝 Sample classes:\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_Stripping\n",
            "      • colorResult_grey shapeResult_ToothMarks textureResult_dark thicknessResult_ecchymosis\n",
            "\n",
            "🎯 DATASET FOUND AT: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1\n",
            "✅ This is where your 9,890 tongue images are stored!\n",
            "\n",
            "📋 Use this path for simplified dataset creation:\n",
            "correct_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
            "\n",
            "🎯 Manual exploration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simplified dataset using the confirmed correct dataset path\n",
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "# Use the confirmed dataset path\n",
        "correct_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "simplified_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset'\n",
        "\n",
        "print(\"🔄 Creating simplified color_shape dataset...\")\n",
        "print(f\"📂 Source: {correct_dataset_path}\")\n",
        "print(f\"📂 Target: {simplified_dataset_path}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear any existing simplified dataset\n",
        "if os.path.exists(simplified_dataset_path):\n",
        "    shutil.rmtree(simplified_dataset_path)\n",
        "\n",
        "# Create simplified dataset with color_shape strategy\n",
        "total_processed = 0\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    source_split_path = os.path.join(correct_dataset_path, split)\n",
        "    target_split_path = os.path.join(simplified_dataset_path, split)\n",
        "\n",
        "    if os.path.exists(source_split_path):\n",
        "        print(f\"\\n📂 Processing {split} split...\")\n",
        "\n",
        "        simplified_counts = defaultdict(int)\n",
        "        os.makedirs(target_split_path, exist_ok=True)\n",
        "\n",
        "        # Process each original class\n",
        "        class_folders = os.listdir(source_split_path)\n",
        "\n",
        "        for i, class_folder in enumerate(class_folders):\n",
        "            class_path = os.path.join(source_split_path, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                # Extract attributes using the function we defined earlier\n",
        "                color, shape, texture, thickness = extract_primary_attributes(class_folder)\n",
        "\n",
        "                # Create simplified class name: color_shape\n",
        "                simplified_class = f\"color_{color}_shape_{shape}\"\n",
        "\n",
        "                # Create target directory\n",
        "                target_class_path = os.path.join(target_split_path, simplified_class)\n",
        "                os.makedirs(target_class_path, exist_ok=True)\n",
        "\n",
        "                # Copy images\n",
        "                copied_count = 0\n",
        "                for image_file in os.listdir(class_path):\n",
        "                    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        source_image = os.path.join(class_path, image_file)\n",
        "                        # Create unique filename to avoid conflicts\n",
        "                        unique_filename = f\"{class_folder.replace(' ', '_')}_{image_file}\"\n",
        "                        target_image = os.path.join(target_class_path, unique_filename)\n",
        "                        shutil.copy2(source_image, target_image)\n",
        "                        copied_count += 1\n",
        "\n",
        "                simplified_counts[simplified_class] += copied_count\n",
        "                total_processed += copied_count\n",
        "\n",
        "                # Progress indicator\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    print(f\"    ✅ Processed {i + 1}/{len(class_folders)} classes...\")\n",
        "\n",
        "        # Show results for this split\n",
        "        print(f\"  📊 Created {len(simplified_counts)} simplified classes:\")\n",
        "        for class_name, count in sorted(simplified_counts.items()):\n",
        "            print(f\"    • {class_name}: {count} images\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\n🎯 SIMPLIFIED DATASET SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "total_images = 0\n",
        "all_classes = set()\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_path = os.path.join(simplified_dataset_path, split)\n",
        "    if os.path.exists(split_path):\n",
        "        split_images = 0\n",
        "        classes = [d for d in os.listdir(split_path)\n",
        "                  if os.path.isdir(os.path.join(split_path, d))]\n",
        "\n",
        "        for class_folder in classes:\n",
        "            all_classes.add(class_folder)\n",
        "            class_path = os.path.join(split_path, class_folder)\n",
        "            image_count = len([f for f in os.listdir(class_path)\n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            split_images += image_count\n",
        "\n",
        "        total_images += split_images\n",
        "        print(f\"📊 {split}: {split_images} images across {len(classes)} classes\")\n",
        "\n",
        "print(f\"\\n🏆 FINAL RESULTS:\")\n",
        "print(f\"📈 Total Images: {total_images}\")\n",
        "print(f\"🏷️ Total Unique Classes: {len(all_classes)}\")\n",
        "print(f\"📊 Average per class: {total_images//len(all_classes) if all_classes else 0}\")\n",
        "print(f\"💾 Simplified dataset location: {simplified_dataset_path}\")\n",
        "\n",
        "print(f\"\\n📝 Simplified classes created:\")\n",
        "for class_name in sorted(all_classes):\n",
        "    print(f\"  • {class_name}\")\n",
        "\n",
        "if total_images > 8000:  # Expect ~9,890 images\n",
        "    print(f\"\\n✅ SUCCESS! Ready for training with {len(all_classes)}-class simplified dataset!\")\n",
        "    print(f\"🚀 Expected accuracy improvement: 0.21% → 60-80%+\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ Only {total_images} images found - some may have been missed\")\n",
        "\n",
        "print(f\"\\n🎯 Ready for next step: Retrain model with simplified dataset!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "sziwsDNmZHd7",
        "outputId": "57faf5d8-e416-4927-b69a-ad4896b24a5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔄 Creating simplified color_shape dataset...\n",
            "📂 Source: /content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1\n",
            "📂 Target: /content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset\n",
            "============================================================\n",
            "\n",
            "📂 Processing train split...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'extract_primary_attributes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-27006843.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# Extract attributes using the function we defined earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_primary_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Create simplified class name: color_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'extract_primary_attributes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the missing extract_primary_attributes function and create simplified dataset\n",
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "def extract_primary_attributes(class_name):\n",
        "    \"\"\"Extract and simplify the primary attributes from complex class names\"\"\"\n",
        "\n",
        "    # Initialize default values\n",
        "    color = \"unknown\"\n",
        "    shape = \"unknown\"\n",
        "    texture = \"unknown\"\n",
        "    thickness = \"unknown\"\n",
        "\n",
        "    # Extract attributes by splitting and parsing each part\n",
        "    parts = class_name.split(' ')\n",
        "    for part in parts:\n",
        "        if 'colorResult_' in part:\n",
        "            color = part.replace('colorResult_', '')\n",
        "        elif 'shapeResult_' in part:\n",
        "            shape = part.replace('shapeResult_', '')\n",
        "        elif 'textureResult_' in part:\n",
        "            texture = part.replace('textureResult_', '')\n",
        "        elif 'thicknessResult_' in part:\n",
        "            thickness = part.replace('thicknessResult_', '')\n",
        "\n",
        "    return color, shape, texture, thickness\n",
        "\n",
        "# Now create the simplified dataset with the function properly defined\n",
        "correct_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/raw_dataset/Tongue-1'\n",
        "simplified_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset'\n",
        "\n",
        "print(\"🔄 Creating simplified color_shape dataset...\")\n",
        "print(f\"📂 Source: {correct_dataset_path}\")\n",
        "print(f\"📂 Target: {simplified_dataset_path}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear any existing simplified dataset\n",
        "if os.path.exists(simplified_dataset_path):\n",
        "    shutil.rmtree(simplified_dataset_path)\n",
        "\n",
        "# Create simplified dataset with color_shape strategy\n",
        "total_processed = 0\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    source_split_path = os.path.join(correct_dataset_path, split)\n",
        "    target_split_path = os.path.join(simplified_dataset_path, split)\n",
        "\n",
        "    if os.path.exists(source_split_path):\n",
        "        print(f\"\\n📂 Processing {split} split...\")\n",
        "\n",
        "        simplified_counts = defaultdict(int)\n",
        "        os.makedirs(target_split_path, exist_ok=True)\n",
        "\n",
        "        # Process each original class\n",
        "        class_folders = os.listdir(source_split_path)\n",
        "\n",
        "        for i, class_folder in enumerate(class_folders):\n",
        "            class_path = os.path.join(source_split_path, class_folder)\n",
        "            if os.path.isdir(class_path):\n",
        "                # Extract attributes using our function\n",
        "                color, shape, texture, thickness = extract_primary_attributes(class_folder)\n",
        "\n",
        "                # Create simplified class name: color_shape\n",
        "                simplified_class = f\"color_{color}_shape_{shape}\"\n",
        "\n",
        "                # Create target directory\n",
        "                target_class_path = os.path.join(target_split_path, simplified_class)\n",
        "                os.makedirs(target_class_path, exist_ok=True)\n",
        "\n",
        "                # Copy images\n",
        "                copied_count = 0\n",
        "                for image_file in os.listdir(class_path):\n",
        "                    if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                        source_image = os.path.join(class_path, image_file)\n",
        "                        # Create unique filename to avoid conflicts\n",
        "                        unique_filename = f\"{class_folder.replace(' ', '_')}_{image_file}\"\n",
        "                        target_image = os.path.join(target_class_path, unique_filename)\n",
        "                        shutil.copy2(source_image, target_image)\n",
        "                        copied_count += 1\n",
        "\n",
        "                simplified_counts[simplified_class] += copied_count\n",
        "                total_processed += copied_count\n",
        "\n",
        "                # Progress indicator every 50 classes\n",
        "                if (i + 1) % 50 == 0:\n",
        "                    print(f\"    ✅ Processed {i + 1}/{len(class_folders)} classes...\")\n",
        "\n",
        "        # Show results for this split\n",
        "        print(f\"  📊 Created {len(simplified_counts)} simplified classes:\")\n",
        "        for class_name, count in sorted(simplified_counts.items()):\n",
        "            print(f\"    • {class_name}: {count} images\")\n",
        "\n",
        "# Final verification\n",
        "print(f\"\\n🎯 SIMPLIFIED DATASET SUMMARY:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "total_images = 0\n",
        "all_classes = set()\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    split_path = os.path.join(simplified_dataset_path, split)\n",
        "    if os.path.exists(split_path):\n",
        "        split_images = 0\n",
        "        classes = [d for d in os.listdir(split_path)\n",
        "                  if os.path.isdir(os.path.join(split_path, d))]\n",
        "\n",
        "        for class_folder in classes:\n",
        "            all_classes.add(class_folder)\n",
        "            class_path = os.path.join(split_path, class_folder)\n",
        "            image_count = len([f for f in os.listdir(class_path)\n",
        "                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            split_images += image_count\n",
        "\n",
        "        total_images += split_images\n",
        "        print(f\"📊 {split}: {split_images} images across {len(classes)} classes\")\n",
        "\n",
        "print(f\"\\n🏆 FINAL RESULTS:\")\n",
        "print(f\"📈 Total Images: {total_images}\")\n",
        "print(f\"🏷️ Total Unique Classes: {len(all_classes)}\")\n",
        "print(f\"📊 Average per class: {total_images//len(all_classes) if all_classes else 0}\")\n",
        "print(f\"💾 Simplified dataset location: {simplified_dataset_path}\")\n",
        "\n",
        "print(f\"\\n📝 Simplified classes created:\")\n",
        "for class_name in sorted(all_classes):\n",
        "    print(f\"  • {class_name}\")\n",
        "\n",
        "if total_images > 8000:\n",
        "    print(f\"\\n✅ SUCCESS! Ready for training with {len(all_classes)}-class simplified dataset!\")\n",
        "    print(f\"🚀 Expected accuracy improvement: 0.21% → 60-80%+\")\n",
        "else:\n",
        "    print(f\"\\n⚠️ Only {total_images} images found - investigating...\")\n",
        "\n",
        "print(f\"\\n🎯 Ready for retraining with much better performance expected!\")\n"
      ],
      "metadata": {
        "id": "-yItai5tZjL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain the DenseNet-121 model with the simplified 12-class dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "print(\"🚀 Setting up training with simplified 12-class dataset...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enhanced data transformations for the simplified dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the simplified dataset\n",
        "simplified_dataset_path = '/content/drive/MyDrive/Tongue_Disease_AI/simplified_dataset'\n",
        "train_path = os.path.join(simplified_dataset_path, 'train')\n",
        "valid_path = os.path.join(simplified_dataset_path, 'valid')\n",
        "test_path = os.path.join(simplified_dataset_path, 'test')\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_path, transform=transform_train)\n",
        "val_dataset = datasets.ImageFolder(root=valid_path, transform=transform_val)\n",
        "test_dataset = datasets.ImageFolder(root=test_path, transform=transform_val)\n",
        "\n",
        "# Create data loaders with larger batch size (more manageable dataset)\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Get dataset information\n",
        "num_classes = len(train_dataset.classes)\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "print(f\"✅ Simplified dataset loaded successfully!\")\n",
        "print(f\"📈 Training samples: {len(train_dataset)}\")\n",
        "print(f\"📊 Validation samples: {len(val_dataset)}\")\n",
        "print(f\"🧪 Test samples: {len(test_dataset)}\")\n",
        "print(f\"🏷️ Number of classes: {num_classes}\")\n",
        "\n",
        "print(f\"\\n📝 Simplified classes:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"{i+1:2d}. {class_name}\")\n",
        "\n",
        "# Initialize new model for simplified classification\n",
        "class SimplifiedTongueClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(SimplifiedTongueClassifier, self).__init__()\n",
        "\n",
        "        # Load pre-trained DenseNet-121\n",
        "        self.densenet = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "\n",
        "        # Get number of features\n",
        "        num_features = self.densenet.classifier.in_features\n",
        "\n",
        "        # Simpler classifier for 12 classes (less overfitting risk)\n",
        "        self.densenet.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(num_features, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.densenet(x)\n",
        "\n",
        "# Initialize the simplified model\n",
        "print(f\"\\n🤖 Initializing simplified tongue classifier...\")\n",
        "model = SimplifiedTongueClassifier(num_classes=num_classes, pretrained=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training components optimized for simplified dataset\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
        "\n",
        "print(f\"✅ Simplified model initialized!\")\n",
        "print(f\"📊 Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"🎯 Ready for training on {num_classes} classes!\")\n",
        "print(f\"\\n🚀 Expected performance: 60-80%+ accuracy (vs previous 0.21%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFVtv7kqZu9b",
        "outputId": "c5b00277-540d-467d-af32-7d670c55c040"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Setting up training with simplified 12-class dataset...\n",
            "============================================================\n",
            "Using device: cpu\n",
            "✅ Simplified dataset loaded successfully!\n",
            "📈 Training samples: 7929\n",
            "📊 Validation samples: 973\n",
            "🧪 Test samples: 988\n",
            "🏷️ Number of classes: 12\n",
            "\n",
            "📝 Simplified classes:\n",
            " 1. color_grey_shape_ToothMarks\n",
            " 2. color_grey_shape_fat\n",
            " 3. color_grey_shape_normal\n",
            " 4. color_grey_shape_thin\n",
            " 5. color_white_shape_ToothMarks\n",
            " 6. color_white_shape_fat\n",
            " 7. color_white_shape_normal\n",
            " 8. color_white_shape_thin\n",
            " 9. color_yellow_shape_ToothMarks\n",
            "10. color_yellow_shape_fat\n",
            "11. color_yellow_shape_normal\n",
            "12. color_yellow_shape_thin\n",
            "\n",
            "🤖 Initializing simplified tongue classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 77.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Simplified model initialized!\n",
            "📊 Total parameters: 7,219,340\n",
            "🎯 Ready for training on 12 classes!\n",
            "\n",
            "🚀 Expected performance: 60-80%+ accuracy (vs previous 0.21%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function optimized for the 12-class simplified dataset\n",
        "def train_simplified_tongue_classifier(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    \"\"\"Training function optimized for simplified 12-class tongue diagnosis\"\"\"\n",
        "\n",
        "    print(f\"🏋️ Starting training for {num_epochs} epochs on simplified dataset...\")\n",
        "    print(f\"🎯 Training: {len(train_dataset)} samples | Validation: {len(val_dataset)} samples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [],\n",
        "        'train_acc': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_path = '/content/drive/MyDrive/Tongue_Disease_AI/models/best_simplified_classifier.pth'\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # Progress every 30 batches\n",
        "            if (batch_idx + 1) % 30 == 0:\n",
        "                batch_acc = torch.sum(preds == labels.data).double() / inputs.size(0)\n",
        "                print(f'    Batch {batch_idx+1}/{len(train_loader)}: Loss={loss.item():.4f}, Acc={batch_acc:.4f}')\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_running_corrects = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                val_running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        val_loss = val_running_loss / len(val_dataset)\n",
        "        val_acc = val_running_corrects.double() / len(val_dataset)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        # Save metrics\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(epoch_acc.item())\n",
        "        history['val_acc'].append(val_acc.item())\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"📈 Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} ({epoch_acc*100:.2f}%)\")\n",
        "        print(f\"📊 Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "                'class_names': class_names,\n",
        "                'num_classes': num_classes,\n",
        "                'history': history\n",
        "            }, best_model_path)\n",
        "            print(f\"🎯 New best model saved! Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "\n",
        "        # GPU memory cleanup every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"🏆 Training completed! Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "    print(f\"🚀 Improvement: 0.21% → {best_val_acc*100:.2f}% (Expected: 60-80%+)\")\n",
        "    print(f\"💾 Best model saved to: {best_model_path}\")\n",
        "\n",
        "    return history, best_model_path\n",
        "\n",
        "# Start training the simplified classifier\n",
        "print(\"🎯 Starting training of simplified 12-class tongue diagnosis system...\")\n",
        "training_history, best_model_path = train_simplified_tongue_classifier(\n",
        "    model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "    num_epochs=25\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Simplified tongue classifier training completed!\")\n",
        "print(f\"🎉 Ready for evaluation and real-world prediction!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "VpBZx5_afoh_",
        "outputId": "4b3bba44-739c-4c91-e938-7952246c4fc0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Starting training of simplified 12-class tongue diagnosis system...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3485808950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🎯 Starting training of simplified 12-class tongue diagnosis system...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m training_history, best_model_path = train_simplified_tongue_classifier(\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix GPU detection and optimize training for T4 GPU\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Force clear any GPU cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Check current GPU status\n",
        "print(\"🔍 GPU Status Check:\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"Current Device: {torch.cuda.current_device()}\")\n",
        "else:\n",
        "    print(\"❌ PyTorch cannot detect GPU\")\n",
        "\n",
        "# Force set device to CUDA if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\n🎯 Setting device to: {device}\")\n",
        "\n",
        "# Test GPU with a simple operation\n",
        "if torch.cuda.is_available():\n",
        "    test_tensor = torch.randn(1000, 1000).to(device)\n",
        "    result = torch.mm(test_tensor, test_tensor.t())\n",
        "    print(f\"✅ GPU test successful! Tensor on device: {result.device}\")\n",
        "    del test_tensor, result\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"❌ GPU test failed - using CPU\")\n",
        "\n",
        "print(f\"\\n🚀 Ready to use {device} for training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tffYeAFgEYm",
        "outputId": "a41aa166-e281-4c47-8a10-7fc5d9aba481"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 GPU Status Check:\n",
            "CUDA Available: False\n",
            "CUDA Version: 12.4\n",
            "PyTorch Version: 2.6.0+cu124\n",
            "❌ PyTorch cannot detect GPU\n",
            "\n",
            "🎯 Setting device to: cpu\n",
            "❌ GPU test failed - using CPU\n",
            "\n",
            "🚀 Ready to use cpu for training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdxrdbm9qXlh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}